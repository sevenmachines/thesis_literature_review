\section{Multi-agent reinforcement learning (MARL)}
\label{section:local_neighbourhoods:background}
To provide some context for the work to follow we look at some relevant research in multi-agent reinforcement learning (MARL). Although there are other useful strategies, such as auction-based systems, and particle swarm optimisation techniques, these also have specific challenges. Auction-based systems carry increasing orchestration cost as the number of agents involved increases, which impacts the scalability of related solutions. They also suffer significant impact when the system is dynamic as agent communication is lost. Swarm approaches can be effective under dynamic conditions but are also prone to optimising on local-optima \cite{Singhal2015}. As we look for an approach that can handle scaling, and dynamic systems, we focus here on MARL. In particular, we look at methods of allocating rewards to drive behaviours, how allocation effects both the exploration of state space, and coordination between agents. 

\subsubsection*{State space exploration in multi-agent reinforcement learning}
\label{section:background:marl}
Multi-agent reinforcement learning (MARL) \cite{Busoniu2008a,Busoniu2010,Tuyls2012} applies reinforcement learning techniques to multiple agents sharing a common environment. Each senses the environment and takes actions that cause a transition of the environment state to a new state, resulting in feedback in the form of the reward signal. There are two main issues that can limit the applicability of MARL techniques. 

Firstly, the exploration of large state-action spaces. As the state space size can exponentially increase in realistic scenarios, finding the right balance of exploration, so that agents' can fully explore the expansive state space, and exploitation, so that they can successfully complete tasks, is difficult. The dimensionality of the system greatly increases with the number of agents, mainly due to the corresponding increases in the number of actions and states. An agent may not only have to learn about its own effects on the environment but also about the nature of other agents in the system. The exploration/exploitation issue increases in difficulty with both a non-stationary environment and the dynamism of other agents policies and actions. 

Secondly, we need to assign credit for task outcomes to specific agents and actions. Since the rewards and values of actions result from multiple agents' contributions, it is difficult to share rewards fairly as the effects of individual actions are not easily separable. The delay between an action and a successful outcome results in a \textit{temporal credit assignment problem} as discussed by Sutton et al \cite{Sutton1984}. There is the additional issue of assigning rewards to individual agents in a collection of agents participating in an outcome, the \textit{structural credit assignment problem} \cite{Zhong2003, Agogino2005}. The difficulty in assigning credit makes choosing a good reward function for the system complex \cite{Mao2020}. We must understand \textit{alignment}, how well the individual agents' own goal optimisation improves the overall system goal. Also, \textit{sensitivity}, how responsive the reward is to an agent changing its own actions. If a reward is sensitive then the agent can separate the effect of changes to its behaviour from the behaviour of other agents more easily. This means it can learn much quicker than when the impact of its actions is less clear. If we use \textit{system rewards}, where system-wide knowledge is used to decide rewards, learning becomes tightly coupled to the actions of other agents in the system, leading to low-sensitivity \cite{WOLPERT2001}. If we use \textit{local rewards}, where we restrict reward calculation to a agents' local-view only, we keep this coupling low. There is a risk however that the agents' behaviours could become non-beneficial to the system goal, or become stuck in local-minima solutions that are sub-optimal. 
\subsubsection*{Coordination in agent-based systems}
Agents in MARL systems can range from being fully cooperative to fully competitive. In cooperative systems the agents all share a common reward function and try to maximise that shared value function. Dedicated algorithms often rely on static, deterministic, or on exact knowledge of other agents states and actions. Coordination and maximisation of joint-action states results in high dimensionality due to the inclusion of other agents actions in calculations. We can utilise the sparseness of the interactions in large multi-agent systems to reduce the coupling between agents by having them work independently and only collecting information about other agents when required. For example, by learning the states where some degree of coordination is required \cite{Melo2009, DeHauwere2010, DeHauwere2012}. In general, coordination in multi-agent systems increases the optimality of solutions found, but at the cost of increased overhead which limits scalability.

This past research highlights some of the key challenges that we look to tackle in our work,
\begin{enumerate}
	\item in large or complex systems the correct policies for agents' behaviour are not known at system initialisation, and may be constantly changing due to system dynamics.
	\item since systems may be dynamic, the optimal solution may be constantly changing.
	\item for a scalable system, system-wide knowledge is not feasible to maintain or to compute with.
	\item agents have physical constraints on compute and memory in real situations that limit their maximum resource usage.
\end{enumerate}
To do this we need to develop the abilities for agents to,
\begin{enumerate}
	\item learn to make the best decisions given their current state.
	\item adapt how they explore state-space depending on how successful they are in task-allocation currently.
	\item make decisions based only on a localised or otherwise partial-view of the system.
	\item must maintain their resource usage within set limits.
\end{enumerate}
The four algorithms we present in the following sections are designed to tackle these issues and combine to form a scalable, resilient, and adaptive mult-agent task allocation solution.